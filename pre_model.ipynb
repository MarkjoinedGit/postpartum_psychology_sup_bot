{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama c++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 387 tensors from models\\PhoGPT-4B-Chat-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = mpt\n",
      "llama_model_loader: - kv   1:                               general.name str              = PhoGPT-4B-Chat\n",
      "llama_model_loader: - kv   2:                         mpt.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                       mpt.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                            mpt.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                    mpt.feed_forward_length u32              = 12288\n",
      "llama_model_loader: - kv   6:                   mpt.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv   7:           mpt.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   8:               mpt.attention.max_alibi_bias f32              = 8.000000\n",
      "llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,20480]   = [\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"!\"...\n",
      "llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,20480]   = [3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,20266]   = [\"á »\", \"á º\", \"Ġ t\", \"n g\", \"Ġ...\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 3\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  258 tensors\n",
      "llama_model_loader: - type q4_K:   80 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 4\n",
      "llm_load_vocab: token to piece cache size = 0.0998 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = mpt\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 20480\n",
      "llm_load_print_meta: n_merges         = 20266\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 24\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 8.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 12288\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = -1\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.69 B\n",
      "llm_load_print_meta: model size       = 2.20 GiB (5.13 BPW) \n",
      "llm_load_print_meta: general.name     = PhoGPT-4B-Chat\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 3 '<pad>'\n",
      "llm_load_print_meta: LF token         = 130 'Ä'\n",
      "llm_load_print_meta: max token length = 328\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2254.62 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.08 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   112.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1191\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'PhoGPT-4B-Chat', 'mpt.attention.head_count': '24', 'general.architecture': 'mpt', 'mpt.context_length': '8192', 'mpt.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'mpt.block_count': '32', 'mpt.feed_forward_length': '12288', 'mpt.attention.layer_norm_epsilon': '0.000010', 'mpt.attention.max_alibi_bias': '8.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.padding_token_id': '3'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"models\\PhoGPT-4B-Chat-Q4_K_M.gguf\",\n",
    "      n_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class Str_OutputParser(StrOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        return self.extract_answer(text)\n",
    "    \n",
    "    def extract_answer(self, \n",
    "                       text_response: str,\n",
    "                       pattern: str = r\"### Trả lời:\\s*(.*)\"\n",
    "                       )->str:\n",
    "        match = re.search(pattern, text_response, re.DOTALL)\n",
    "        if match:\n",
    "            answer_text = match.group(1).strip()\n",
    "            return answer_text\n",
    "        else:\n",
    "            return text_response\n",
    "        \n",
    "class Offline_RAG:\n",
    "    def __init__(self, llm) -> None:\n",
    "        self.llm = llm\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Bạn là trợ lý AI hỗ trợ về sức khoẻ tâm lý sau sinh. Dựa vào ngữ cảnh bên dưới hãy đưa ra câu trả lời và lời khuyên phù hợp cho câu hỏi của họ. Nếu bạn không biết câu trả lời, hãy nói không biết, đừng cố tạo ra câu trả lời. \\n# Ngữ cảnh: \\n{context}\\n## Câu hỏi: {question}\\n### Trả lời:\"\"\"\n",
    "        )\n",
    "        self.str_parser = Str_OutputParser()\n",
    "\n",
    "    def get_output(self, retriever, question):\n",
    "        input_data = {\n",
    "            \"context\":  self.format_docs(retriever.invoke(question)),\n",
    "            \"question\": question\n",
    "        }\n",
    "        print(input_data)\n",
    "        self.message = self.prompt.format(**input_data)\n",
    "        output = self.llm(\n",
    "            self.message,\n",
    "            max_tokens=2048,\n",
    "            echo=True\n",
    "        )\n",
    "        #print(output[\"choices\"][0][\"text\"])\n",
    "        return self.str_parser.parse(output[\"choices\"][0][\"text\"])\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "from pinecone import Pinecone\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self,\n",
    "                 embedding = HuggingFaceEmbeddings()) -> None:\n",
    "        os.environ['PINECONE_API_KEY'] = \"73bdec39-1f93-47fc-bd2f-f02883d7be83\"\n",
    "        self.pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "        self.embedding = embedding\n",
    "        self.db = self._build_db()\n",
    "    \n",
    "    def _build_db(self):\n",
    "        db = PineconeVectorStore.from_existing_index(\n",
    "            index_name=\"docs-rag-chatbot\",\n",
    "            namespace=\"docs-search\",\n",
    "            embedding=self.embedding\n",
    "        )\n",
    "        return db\n",
    "    \n",
    "    # tính similarity\n",
    "    def get_retriever(self,\n",
    "                      search_type: str = \"similarity\",\n",
    "                      search_kwargs: dict = {\"k\": 5}\n",
    "                      ):\n",
    "        retriever =self.db.as_retriever(search_type=search_type,\n",
    "                                        search_kwargs=search_kwargs)\n",
    "        return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'người có tiền sử bị trầm cảm thì dễ tái phát sau sinh, những sự kiện stress trong quá trình mang thai hoặc trong quá trình sinh con, gặp khó khăn trong khi sinh như đẻ khó, con khi sinh ra gặp phải những vấn đề về sức khỏe, sinh non. Mặt khác, trẻ có mẹ bị trầm cảm sau sinh có thể có những ảnh hưởng nhất định. Trẻ có các hành vi bất thường như giấc ngủ không ngon giấc, hành vi dễ bị kích động và tăng động.Chậm phát triển về nhận thức, chậm nói, chậm đi hơn những trẻ khác. Chúng cũng có thể gặp\\n\\n16 \\n \\nlực lên phụ nữ trong việc sinh con trai và có thể ảnh hưởng nghiêm trọng đến \\nsức khỏe tâm thần của phụ nữ trong thời gian mang thai [55]. \\n\\uf0d8 Stress trong mang thai \\nStress được đo bằng nhiều cách khác nhau như các sự kiện quan trọng xảy \\nra trong đời sống cá nhân như ly hôn hoặc tử vong của người thân trong gia đình. \\nNghiên cứu của Lancaster và cộng sự năm 2010 tổng hợp trên 20 bài báo đã chỉ \\nra các sự kiện tiêu cực trong cuộc sống làm tăng nguy cơ bị trầm cảm [16].\\n\\nthấy những thai phụ bị stress có nguy cơ bị trầm cảm cao gấp 3 lần so với \\nnhững người không bị stress  [16].  \\n\\uf0d8 Tiền sử trầm cảm   \\nMột số nghiên cứu đã tổng hợp và cho ra kết quả tiền sử trầm cảm làm \\ntăng nguy cơ trầm cảm trong mang thai. Nghiên cứu của Lancaster và cộng sự \\nnăm 2010 và nghiên cứu của Kesler năm 2013 cho thấy phụ nữ có tiền sử \\ntrầm cảm trước khi mang thai có mối liên quan chặt chẽ đến trầm cảm trong \\nmang thai [16], [40]. \\n\\uf0d8 Hỗ trợ xã hội\\n\\n108 \\n \\ntắm trong tháng đầu sau sinh và không cho phụ nữ và em bé ra ngoài vì sợ \\nnắng và gió. Bốn phụ nữ chia sẻ họ cảm thấy ngột ngạt khi ở mãi trong phòng \\nkín, nhiều khi làm cho mình cảm thấy bế tắc và tù túng. Điều này, khiến một \\nsố phụ nữ có thể bị stress lâu dài sẽ phát triển thành trầm cảm [54]. \\n4.3.3. Hành vi bạo lực của chồng đối với phụ nữ \\nKết quả cho thấy có gần 59% phụ nữ cho rằng chồng của họ là người \\nquan trọng trong việc hỗ trợ những việc hàng ngày trong khi mang thai và sau\\n\\ntrong gia đình [80].  \\nỞ Malaysia, phụ nữ phải tuân thủ những quy định nghiêm ngặt về chế \\nđộ ăn kiêng và phải mặc quần áo ấm, buộc đá ấm xung quanh bụng hoặc ở \\ntrong một căn phòng nóng để phục hồi cân bằng cơ thể và để tự bảo vệ mình \\nkhỏi bị tổn hại bởi những linh hồn ma quỷ [81]. Việc tuân thủ các phong tục \\ntập quán sau sinh ở các nền văn hóa khác nhau khiến một số phụ nữ có thể bị \\nstress, lâu ngày có thể dẫn đến trầm cảm [54]. Như nghiên cứu cắt ngang của', 'question': 'Stress sau sinh và trầm cảm sau sinh khác nhau như thế nào?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   49881.49 ms\n",
      "llama_print_timings:      sample time =      15.82 ms /   285 runs   (    0.06 ms per token, 18019.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   77365.31 ms /   793 tokens (   97.56 ms per token,    10.25 tokens per second)\n",
      "llama_print_timings:        eval time =   49850.92 ms /   284 runs   (  175.53 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =  127635.45 ms /  1077 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress sau sinh và trầm cảm sau sinh là hai tình trạng riêng biệt liên quan đến những thay đổi về tâm lý và sinh lý xảy ra sau khi mang thai và sinh con. Stress sau sinh là sự căng thẳng phát triển sau khi mang thai và sinh con, trong khi trầm cảm sau sinh là một tình trạng liên quan đến sự lo lắng, căng thẳng và cảm xúc tiêu cực có thể ảnh hưởng đến cả mẹ và em bé. Các triệu chứng của mỗi tình trạng có thể khác nhau tùy thuộc vào mức độ nghiêm trọng của các vấn đề tâm lý của mẹ và tác động của các yếu tố khác nhau, chẳng hạn như căng thẳng trong quá trình mang thai, sinh con và những thay đổi trong cuộc sống gia đình và xã hội. Những người có tiền sử bị trầm cảm có thể dễ bị tái phát sau sinh và có thể gặp khó khăn hơn trong việc đối phó với những tình huống căng thẳng và áp lực. Mặt khác, những người có mẹ bị trầm cảm sau sinh có thể có những ảnh hưởng nhất định, bao gồm chậm phát triển về nhận thức, chậm nói và chậm đi hơn những trẻ khác. Trẻ có mẹ bị trầm cảm sau sinh có thể có những hành vi bất thường như giấc ngủ không ngon giấc, hành vi dễ bị kích động và tăng động. Chúng cũng có thể gặp các vấn đề về sức khỏe tâm thần, bao gồm trầm cảm và lo lắng.\n"
     ]
    }
   ],
   "source": [
    "retriever = VectorDB().get_retriever()\n",
    "output = Offline_RAG(llm).get_output(retriever,\"Stress sau sinh và trầm cảm sau sinh khác nhau như thế nào?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   42158.51 ms\n",
      "llama_print_timings:      sample time =       3.21 ms /    70 runs   (    0.05 ms per token, 21806.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1049.16 ms /    17 tokens (   61.72 ms per token,    16.20 tokens per second)\n",
      "llama_print_timings:        eval time =    8543.49 ms /    69 runs   (  123.82 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =    9646.06 ms /    86 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trầm cảm là một tình trạng tâm lý biểu hiện bằng cảm giác buồn, mất hứng, cảm thấy không có động lực và khó chịu. Tình trạng này có thể ảnh hưởng đến tâm trạng và chất lượng cuộc sống của một người. Trầm cảm có thể kéo dài hoặc ngắn hạn và có thể được điều trị bằng thuốc và liệu pháp.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"## Câu hỏi: {question}\\n### Trả lời:\"\"\"\n",
    ")\n",
    "messages = prompt_template.format(question=\"Trầm cảm là gì?\")\n",
    "output = llm(\n",
    "      messages, # Prompt\n",
    "      max_tokens=1024, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"## Câu hỏi:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output[\"choices\"][0][\"text\"]) # Print the generated completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
