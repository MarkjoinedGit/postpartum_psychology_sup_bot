{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tải xuống: 100.00%\n",
      "Tệp đã được tải xuống và lưu tại models\\Llama-3.1-MedPalm2-imitate-8B-Instruct.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = 'https://huggingface.co/mradermacher/Llama-3.1-MedPalm2-imitate-8B-Instruct-GGUF/resolve/main/Llama-3.1-MedPalm2-imitate-8B-Instruct.Q8_0.gguf?download=true'\n",
    "save_directory = 'models'\n",
    "filename = 'Llama-3.1-MedPalm2-imitate-8B-Instruct.Q8_0.gguf'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(save_directory, filename)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f'Tệp {filename} đã tồn tại trong thư mục {save_directory}.')\n",
    "else:\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get('content-length', 0))  # Total size in bytes\n",
    "        block_size = 8192  # Download in chunks of 8KB\n",
    "        progress = 0  # Track the progress\n",
    "\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    progress += len(chunk)\n",
    "                    percent_complete = (progress / total_size) * 100\n",
    "                    print(f'Tải xuống: {percent_complete:.2f}%', end='\\r')\n",
    "\n",
    "        print(f'\\nTệp đã được tải xuống và lưu tại {file_path}')\n",
    "    else:\n",
    "        print('Có lỗi xảy ra trong quá trình tải xuống.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Llama.cpp server (run on zeppelin)\n",
    "**Lệnh xem các thông số:** *python -m llama_cpp.server --help*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 33 key-value pairs and 464 tensors from models/gemma-2-9b-it-Q3_K_L.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2-9b-it\n",
      "llama_model_loader: - kv   2:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                    gemma2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv   4:                         gemma2.block_count u32              = 42\n",
      "llama_model_loader: - kv   5:                 gemma2.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                gemma2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:             gemma2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  10:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  12:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  13:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  14:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/gemma-2-9b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 294\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  169 tensors\n",
      "llama_model_loader: - type q3_K:  168 tensors\n",
      "llama_model_loader: - type q5_K:  126 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 217\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 42\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 9B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 9.24 B\n",
      "llm_load_print_meta: model size       = 4.77 GiB (4.44 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2-9b-it\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.21 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4888.91 MiB\n",
      ".............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 16384\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  5376.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 5376.00 MiB, K (f16): 2688.00 MiB, V (f16): 2688.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   606.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1690\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'gemma2.attention.head_count': '16', 'general.name': 'gemma-2-9b-it', 'general.architecture': 'gemma2', 'gemma2.context_length': '8192', 'gemma2.attention.head_count_kv': '8', 'gemma2.embedding_length': '3584', 'gemma2.block_count': '42', 'gemma2.feed_forward_length': '14336', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma2.attn_logit_softcapping': '50.000000', 'general.file_type': '13', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.sliding_window': '4096', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models_out/gemma-2-9b-it-GGUF/gemma-2-9b-it.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '294'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "INFO:     Started server process [10176]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:7500 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50770 - \"GET / HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [10176]\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp.server.app import create_app\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "from llama_cpp.server.app import create_app\n",
    "os.environ[\"MODEL\"] = \"models/gemma-2-9b-it-Q3_K_L.gguf\"\n",
    "os.environ[\"N_CTX\"] = \"16384\"\n",
    "\n",
    "app = create_app()\n",
    "\n",
    "# Apply nest_asyncio to allow running async functions in an existing loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Running uvicorn inside an existing event loop\n",
    "async def start_server():\n",
    "    uvicorn.run(\n",
    "        app, host=os.getenv(\"HOST\", \"0.0.0.0\"), port=int(os.getenv(\"PORT\", 7500)), log_config = None\n",
    "    )\n",
    "\n",
    "asyncio.run(start_server())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call model via API (llama_cpp server) with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(base_url=\"http://zep.hcmute.fit/7500/v1\", api_key=\"llama.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Offline_RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "class Offline_RAG:\n",
    "    def __init__(self, llm) -> None:\n",
    "        self.llm = llm\n",
    "\n",
    "        self.history_chat =[]\n",
    "        \n",
    "        condense_question_template = \"\"\"\n",
    "        Với đoạn hội thoại sau và một câu hỏi tiếp theo, hãy diễn đạt lại câu hỏi tiếp theo để nó trở thành một câu hỏi độc lập.\n",
    "        \n",
    "        Lịch sử hội thoại:\n",
    "        {chat_history}\n",
    "        Câu hỏi tiếp theo: {question}\n",
    "        Câu hỏi độc lập:\"\"\"\n",
    "        \n",
    "        self.condense_question_prompt = ChatPromptTemplate.from_template(condense_question_template)\n",
    "\n",
    "        qa_template = \"\"\"\n",
    "        Bạn là trợ lý AI hỗ trợ về sức khoẻ tâm lý sau sinh. \n",
    "        Dựa vào nội dung gợi ý trả lời bên dưới, hãy đưa ra câu trả lời và lời khuyên phù hợp cho câu hỏi của họ.\n",
    "        Nếu bạn không biết câu trả lời, hãy nói không biết, đừng cố tạo ra câu trả lời.\n",
    "\n",
    "        Lịch sử hội thoại:\n",
    "        {chat_history}\n",
    "\n",
    "        Nội dung gợi ý trả lời:\n",
    "        {context}\n",
    "\n",
    "        Câu hỏi: {question}\n",
    "        \"\"\"\n",
    "\n",
    "        self.qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "    def get_chain(self, retriever):\n",
    "        rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm,\n",
    "            retriever,\n",
    "            condense_question_prompt=self.condense_question_prompt,\n",
    "            combine_docs_chain_kwargs={\n",
    "                \"prompt\": self.qa_prompt,\n",
    "            },\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "        return rag_chain\n",
    "    \n",
    "    def add_history(self, user_chat, bot_chat):\n",
    "        self.history_chat.append((\"user\", user_chat))\n",
    "        self.history_chat.append((\"assistant\", bot_chat))\n",
    "        if len(self.history_chat) > 5:\n",
    "            self.history_chat.pop(0)\n",
    "    \n",
    "    def get_input_data(self,question):\n",
    "        return {\n",
    "            \"chat_history\": self.history_chat,\n",
    "            \"question\": question\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the VectorDB class using Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3268\\3425626517.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings()) -> None:\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\chatbot\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self,\n",
    "                 embedding = HuggingFaceEmbeddings()) -> None:\n",
    "        os.environ['PINECONE_API_KEY'] = \"73bdec39-1f93-47fc-bd2f-f02883d7be83\"\n",
    "        self.pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "        self.embedding = embedding\n",
    "        self.db = self._build_db()\n",
    "    \n",
    "    def _build_db(self):\n",
    "        db = PineconeVectorStore.from_existing_index(\n",
    "            index_name=\"docs-rag-chatbot\",\n",
    "            namespace=\"docs-store\",\n",
    "            embedding=self.embedding\n",
    "        )\n",
    "        return db\n",
    "    \n",
    "    # tính similarity\n",
    "    def get_retriever(self,\n",
    "                      search_type: str = \"similarity\",\n",
    "                      search_kwargs: dict = {\"k\": 5}\n",
    "                      ):\n",
    "        retriever =self.db.as_retriever(search_type=search_type,\n",
    "                                        search_kwargs=search_kwargs)\n",
    "        return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function build chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_chain(llm):\n",
    "    retriever = VectorDB().get_retriever()\n",
    "    off_rag = Offline_RAG(llm)\n",
    "    rag_chain = off_rag.get_chain(retriever)\n",
    "    return rag_chain, off_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not prompt and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Đau đầu, mệt mỏi, suy nghĩ tiêu cực sau sinh là bị bệnh gì?', 'history': '', 'response': 'Chào bạn! Tôi hiểu bạn đang lo lắng về những triệu chứng bạn đang trải qua sau khi sinh nở.  Đau đầu, mệt mỏi và suy nghĩ tiêu cực sau sinh là những triệu chứng rất phổ biến được gọi là **bệnh trầm cảm sau sinh** hoặc **baby blues**. \\n\\nTuy nhiên, tôi không phải là chuyên gia y tế nên tôi không thể chẩn đoán bệnh.  Điều quan trọng nhất là bạn cần đến gặp bác sĩ để được kiểm tra và tư vấn điều trị phù hợp. Bác sĩ có thể giúp bạn xác định nguyên nhân của những triệu chứng này và đưa ra lời khuyên tốt nhất cho trường hợp của bạn. \\n\\nHãy nhớ rằng, bạn không đơn độc và rất nhiều phụ nữ khác cũng trải qua những cảm xúc tương tự sau khi sinh con.  Đừng ngần ngại tìm kiếm sự trợ giúp từ người thân, bạn bè hoặc các chuyên gia y tế.\\n\\n\\n'}\n",
      "Time: 91.38202548027039\n",
      "{'input': 'Hiện tại tôi không có thời gian đi bác sĩ, con tôi cứ khóc hoài, có cách nào để cho nó ngủ ngoan không ?', 'history': 'Human: Đau đầu, mệt mỏi, suy nghĩ tiêu cực sau sinh là bị bệnh gì?\\nAI: Chào bạn! Tôi hiểu bạn đang lo lắng về những triệu chứng bạn đang trải qua sau khi sinh nở.  Đau đầu, mệt mỏi và suy nghĩ tiêu cực sau sinh là những triệu chứng rất phổ biến được gọi là **bệnh trầm cảm sau sinh** hoặc **baby blues**. \\n\\nTuy nhiên, tôi không phải là chuyên gia y tế nên tôi không thể chẩn đoán bệnh.  Điều quan trọng nhất là bạn cần đến gặp bác sĩ để được kiểm tra và tư vấn điều trị phù hợp. Bác sĩ có thể giúp bạn xác định nguyên nhân của những triệu chứng này và đưa ra lời khuyên tốt nhất cho trường hợp của bạn. \\n\\nHãy nhớ rằng, bạn không đơn độc và rất nhiều phụ nữ khác cũng trải qua những cảm xúc tương tự sau khi sinh con.  Đừng ngần ngại tìm kiếm sự trợ giúp từ người thân, bạn bè hoặc các chuyên gia y tế.\\n\\n\\n', 'response': 'Tôi hiểu bạn đang rất mệt mỏi và muốn bé ngủ ngon giấc.  Tuy nhiên, như tôi đã nói trước, tôi không phải là chuyên gia y tế và không thể đưa lời khuyên về chăm sóc trẻ sơ sinh. \\n\\nCó nhiều cách để giúp bé ngủ ngoan hơn, nhưng mỗi em bé đều khác nhau và điều gì hiệu quả với một em bé có thể không hiệu quả với em bé khác.  Bạn có thể thử những mẹo sau:\\n\\n* **Tạo môi trường ngủ yên tĩnh và tối:** Giảm thiểu tiếng ồn và ánh sáng xung quanh nơi bé ngủ.\\n* **Thực hiện thói quen trước khi ngủ:** Tắm nước ấm, massage nhẹ nhàng, hát ru hoặc đọc sách cho bé có thể giúp bé thư giãn và sẵn sàng ngủ.\\n* **Cho bé bú đầy đủ:**  Bé sẽ dễ ngủ hơn nếu đã no nê.\\n* **Dành thời gian chăm sóc bé:** Đôi khi bé khóc vì cần sự quan tâm và gần gũi của mẹ.\\n\\nTuy nhiên, nếu bé khóc liên tục hoặc có dấu hiệu bất thường, bạn nên liên hệ với bác sĩ hoặc chuyên gia chăm sóc trẻ sơ sinh để được tư vấn. \\n\\n\\n'}\n",
      "Time: 160.35926842689514\n",
      "Average time: 125.87064695358276\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "import time\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory\n",
    ")\n",
    "sum_time = 0\n",
    "n = 0\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "\n",
    "    start = time.time()\n",
    "    response = conversation.invoke({\n",
    "            \"input\": question\n",
    "        })\n",
    "    end = time.time()\n",
    "\n",
    "    sum_time += end - start\n",
    "    n += 1\n",
    "\n",
    "    print(response)\n",
    "    \n",
    "    print(f\"Time: {end - start}\")\n",
    "\n",
    "print(f\"Average time: {sum_time/n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Đau đầu, mệt mỏi, suy nghĩ tiêu cực sau sinh là bị bệnh gì?', 'history': '', 'response': 'Tôi hiểu bạn đang cảm thấy rất khó khăn.  Đau đầu, mệt mỏi và suy nghĩ tiêu cực sau sinh là những triệu chứng thường gặp của **thoi ương trầm cảm** (postpartum depression).\\n\\nTuy nhiên, tôi không phải là chuyên gia y tế và không thể chẩn đoán bệnh. \\n\\n**Điều quan trọng nhất là bạn nên liên hệ với bác sĩ hoặc chuyên gia sức khỏe tâm thần để được tư vấn và hỗ trợ phù hợp.** Họ có thể giúp bạn xác định nguyên nhân của những triệu chứng bạn đang gặp phải và đưa ra phương pháp điều trị tốt nhất cho trường hợp của bạn.\\n\\nHãy nhớ rằng bạn không đơn độc và có rất nhiều nguồn lực sẵn sàng hỗ trợ bạn vượt qua giai đoạn này. \\n'}\n",
      "Time: 85.21116423606873\n",
      "{'input': 'Hiện tại tôi không có thời gian đi bác sĩ, con tôi cứ khóc hoài, có cách nào để cho nó ngủ ngoan không ?', 'history': 'Human: Đau đầu, mệt mỏi, suy nghĩ tiêu cực sau sinh là bị bệnh gì?\\nAI: Tôi hiểu bạn đang cảm thấy rất khó khăn.  Đau đầu, mệt mỏi và suy nghĩ tiêu cực sau sinh là những triệu chứng thường gặp của **thoi ương trầm cảm** (postpartum depression).\\n\\nTuy nhiên, tôi không phải là chuyên gia y tế và không thể chẩn đoán bệnh. \\n\\n**Điều quan trọng nhất là bạn nên liên hệ với bác sĩ hoặc chuyên gia sức khỏe tâm thần để được tư vấn và hỗ trợ phù hợp.** Họ có thể giúp bạn xác định nguyên nhân của những triệu chứng bạn đang gặp phải và đưa ra phương pháp điều trị tốt nhất cho trường hợp của bạn.\\n\\nHãy nhớ rằng bạn không đơn độc và có rất nhiều nguồn lực sẵn sàng hỗ trợ bạn vượt qua giai đoạn này. \\n', 'response': 'Tôi hiểu bạn đang trong một tình huống rất khó khăn và mệt mỏi.  Việc chăm sóc một em bé sơ sinh có thể rất áp lực, đặc biệt khi bạn đang cảm thấy đau đầu, mệt mỏi và suy nghĩ tiêu cực. \\n\\nTuy nhiên, tôi không phải là chuyên gia về việc chăm sóc trẻ sơ sinh và không thể đưa ra lời khuyên y tế. \\n\\n**Điều quan trọng là bạn nên ưu tiên sức khỏe của chính mình.** Hãy tìm một người có thể giúp bạn trông trẻ cho một lúc ngắn ngủi để bạn có thể nghỉ ngơi hoặc liên hệ với gia đình, bạn bè hoặc các nhóm hỗ trợ sau sinh để được sự trợ giúp.  \\n'}\n",
      "Time: 116.77633357048035\n",
      "Average time: 100.99374890327454\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "qa_template = \"\"\"\n",
    "        Bạn là trợ lý AI hỗ trợ về sức khoẻ tâm lý sau sinh. \n",
    "        Hãy đưa ra câu trả lời và lời khuyên phù hợp cho câu hỏi của họ.\n",
    "        Nếu bạn không biết câu trả lời, hãy nói không biết, đừng cố tạo ra câu trả lời.\n",
    "\n",
    "        Lịch sử hội thoại:\n",
    "        {history}\n",
    "\n",
    "        Câu hỏi: {input}\n",
    "        \"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    prompt = qa_prompt\n",
    ")\n",
    "\n",
    "sum_time = 0\n",
    "n = 0\n",
    "\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "    start = time.time()\n",
    "    response = conversation.invoke({\n",
    "            \"input\": question\n",
    "        })\n",
    "    end = time.time()\n",
    "    sum_time += end - start\n",
    "    n += 1\n",
    "    print(response)\n",
    "    print(f\"Time: {end - start}\")\n",
    "\n",
    "print(f\"Average time: {sum_time/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': [], 'question': 'Tôi cảm thấy mệt mỏi và căng thẳng, có phải tôi bị stress sau sinh không ?', 'answer': 'Dựa vào những gì bạn mô tả, rất có thể bạn đang gặp phải tình trạng stress sau sinh.  Cảm giác mệt mỏi và căng thẳng là những dấu hiệu thường gặp của stress. \\n\\nHãy nhớ rằng đây chỉ là một gợi ý, để chắc chắn hơn bạn nên tham khảo ý kiến bác sĩ hoặc chuyên gia sức khỏe tâm thần. Họ có thể đánh giá tình hình cụ thể của bạn và đưa ra lời khuyên phù hợp.  \\n', 'source_documents': [Document(metadata={'page': 0.0, 'source': 'data_source\\\\nhung_thay_doi_tam_ly_sau_sinh.pdf'}, page_content='con c ủa mình n ữa, lo s ợ làm t ổn hại con, m ất ham mu ốn tình d ục, mất ngủ, cảm thấy không có giá tr ị và có t ội lỗi, ăn không ngon mi ệng ho ặc sút cân, có ý nghĩ tự sát... Nh ững ngư ời có ti ền sử bị trầm cảm thì d ễ tái phát sau sinh, nh ững sự kiện stress trong quá trình mang thai ho ặc trong quá trình sinh con, g ặp khó khăn trong khi sinh như đ ẻ khó, con khi sinh ra g ặp phải những vấn đề về sức'), Document(metadata={'page': 0.0, 'source': 'data_source\\\\Stress_co_bi_mat_sua_khong_Giai_phap_cho_me_co_sua_sau_sinh.pdf'}, page_content='Stress có bị mất sữa không? Giải pháp cho mẹ có sữa sau sinh Được làm mẹ là một điều cao cả và vĩ đại, vì thế chúng ta cần chuẩn bị thật kỹ trước và sau khi sinh. Đặc biệt, sữa mẹ thật sự rất cần thiết cho quá trình phát triển của trẻ nhỏ và được ưu tiên hàng đầu khi chào đón em bé. Thế nhưng ít ai biết sau sinh các mẹ thường rơi vào trạng thái căng thẳng, stress dẫn đến mất sữa sau sinh. Từ đó câu hỏi được đặt ra liệu stress có bị'), Document(metadata={'page': 3.0, 'source': 'data_source\\\\Stress_Sau_Sinh_Nguyen_Nhan_Dau_Hieu_Va_Cach_Dieu_Tri.pdf'}, page_content='phụ nữ sau sinh giảm thiểu stress và chữa lành cảm xúc. Người thân có thể hỗ trợ và hướng dẫn phụ nữ sau sinh cách sóc em bé tốt nhất; nấu cho các bà mẹ những bữa ăn ngon, đầy dinh dưỡng; lắng nghe, thấu hiểu và chia sẻ để người mẹ sau sinh cảm thấy được động viên, kết nối cảm xúc. Trong đó, vai trò của người chồng được xem là quan trọng nhất, hãy bớt thời gian ở bên ngoài để phụ giúp vợ chăm sóc con, cho vợ có thêm thời gian nghỉ ngơi, quan tâm và trò'), Document(metadata={'page': 1.0, 'source': 'data_source\\\\Nhung_cach_giam_stress_sau_sinh_hieu_qua_ma_ban_nen_biet__YouMed.pdf'}, page_content='nếu không cảm thấy thoải mái với việc này hoặc thấy khó chịu sau khi cho con bú, bạn có thể ngừng lại. Hãy chọn một cách cho bé ăn khác khiến bạn thấy dễ chịu hơn.1 Cho con bú bằng sữa mẹ cũng giúp giảm stress sau sinh'), Document(metadata={'page': 0.0, 'source': 'data_source\\\\Nhung_cach_giam_stress_sau_sinh_hieu_qua_ma_ban_nen_biet__YouMed.pdf'}, page_content='Sau khi có con, các bà mẹ thường xuyên cảm thấy bận rộn với vai trò của mình. Các công việc thường không bao giờ hết. Chăm sóc con cái, công việc riêng, việc nhà…liên tiếp ập tới khiến nhiều người cảm thấy choáng ngợp. Chính vì vậy, họ dễ bị stress và không có thời gian dành')]}\n",
      "Time: 14.386048316955566\n",
      "Average time: 14.386048316955566\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "chain, off_rag = build_rag_chain(llm)\n",
    "sum_time = 0\n",
    "n = 0\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "    \n",
    "    start = time.time()\n",
    "    response = chain.invoke(off_rag.get_input_data(question))\n",
    "    end = time.time()\n",
    "\n",
    "    sum_time += end - start\n",
    "    n += 1\n",
    "\n",
    "    print(response)\n",
    "    \n",
    "    off_rag.add_history(question, response[\"answer\"])\n",
    "\n",
    "    print(f\"Time: {end - start}\")\n",
    "\n",
    "print(f\"Average time: {sum_time/n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
